from janome.tokenizer import Tokenizer

t = Tokenizer()
val = 'Panasonic Beauty SALON éŠ€åº§ã§ã¯ #ç¾å®¹å®¶é›» ã®ä½“é¨“ã ã‘ã§ã¯ãªãã€ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã«ã‚ˆã‚‹ãƒ—ãƒã‚»ãƒŸãƒŠãƒ¼ã‚’å¹³æ—¥æ¯æ—¥é–‹å‚¬âœ¨ï¼ˆäºˆç´„ä¸è¦ï¼‰\n10æœˆã®ãƒ†ãƒ¼ãƒğŸ‚ã¯ã€Œç§‹ã®ãƒˆãƒ¬ãƒ³ãƒ‰ #ãƒã‚¤ãƒ³ãƒˆãƒ¡ã‚¤ã‚¯ã€ã„ã¤ã‚‚ã® #ã‚¢ã‚¤ãƒ¡ã‚¤ã‚¯ ã‚„ #ãƒªãƒƒãƒ— ã‚’å¤‰ãˆã¦ã¿ã¾ã›ã‚“ã‹ï¼Ÿ\n#ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ #panasonicbeauty'
tokens = t.tokenize(val)
data = []
for token in tokens:
    print(token)
    each_data = []
    partOfSpeech = token.part_of_speech.split(',')[0]
    if partOfSpeech == u'åè©':
        print(token.surface)
        if token.surface != "#":
            data.append(token.surface)

print(data)
